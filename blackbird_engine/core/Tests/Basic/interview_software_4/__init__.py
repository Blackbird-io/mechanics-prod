#PROPRIETARY AND CONFIDENTIAL
#Property of Blackbird Logical Applications, LLC
#Copyright Blackbird Logical Applications, LLC 2014
#NOT TO BE CIRCULATED OR REPRODUCED WITHOUT PRIOR WRITTEN APPROVAL OF ILYA PODOLYAKO

#Blackbird Diagnostics
#Module: Tests.Basic.interview_test_template.__init__

#Module follows standard Test interface.
#Module requires that package contain a standard.pkl file.

"""
interview_retail_3

Task starts with a blank portal message and runs through the retail_3_raw
interview script using only the API interface. Grader checks that output
includes the expected primitive info.

Grader does **not** check that e_model matches any particular state. 

Reporting structure: 

    result["testName"]   name of this test
    result["output"]     output object passed in as argument
    result["errors"]     a list of errors, if any, generated during .do()
                         and/or .check()
    result["completed"]  bool, did Test.do() run without generating exception
    result["passed"]     bool, did Grader determine output satisfies standard
    result["rubric"]     obj, optionally supplied by Grader to explain passed
                         status
Module interface:
====================  ==========================================================
Object                Description
====================  ==========================================================
testName              str, should be same as file name (var used for paths)
testPath              str, location of current file; derived from parent package
                      name and ``testName``
Function:

do()                  imports Task script, returns result, including output from
                      Task.do()
check()               checks result from do() against a standard
====================  ==========================================================
"""




#imports
import dill as pickle
import inspect
import os
import sys
import traceback




#globals
testName = "interview_test_template"
testPath = "Tests" + "\\" + "Basic" + "\\" + testName

def do(retainState = False):
    """

    run() -> dict

    ``result`` dictionary is a standard reporting form for tests. result
    contains 6 keys listed above. .do() fills out the values for ``output``
    and ``errors``.
    
    A Task can return any object as ``output``. Task uses output to package and
    transport data that the author of the test deems significant. The test's
    Grader module is the only object that has to know the output's interface. 

    If an exception arises when executing Task, .do() intercepts the exception
    and includes it in ``errors``.

    If ``retainState`` constructor is True, .do() sets result[``output``] to the
    state of Task.output even if an exception occurs. That is, result retains
    the state of Task.

    """
    result = {}
    result["testName"] = testName
    result["output"] = None
    result["errors"] = []
    result["completed"] = False
    result["passed"] = False
    result["rubric"] = None
    print("current build: %s \n" % sys.path[-1])
    print("running %s.do()" % testName)
    output = None
    try:
        from . import Task
        output = Task.do()
        result["output"] = output
        result["completed"] = True
        #calling Task.do() returns a dictionary object that contains
        #whatever Task wants to put in it. Necessary to pass dictionary because
        #pickle module cannot pickle a complete module itself.
    except Exception as F:
        output = F
        print("Test.do encountered an exception: ")
        print(F)
        print("running traceback funcitons")
        print("traceback.print_exc(file=sys.stdout)")
        traceback.print_exc(file=sys.stdout)
        print("traceback.print_stack(file=sys.stdout)")
        traceback.print_stack(file=sys.stdout)
        result["errors"] = [output]
        result["completed"] = False
        if retainState:
            result["output"] = Task.output
        else:
            result["output"] = None
    print("%s generated the following output: \n\t%s" % (testName,output))
    return result
    
def check(result):
    """

    check(result) -> dict

    Function checks whether output generated by Task satisfies standard.
    Builds that generate an error during .do() automatically fail. If .check()
    finds that result["errors"] is filled, it marks the result accordingly and
    skips further analysis.
    """
    print("running %s.check() on result..." % testName)
    #1. get grader
    from . import Grader
    #2. get standard file by piggy-backing on relative imports
    path_for_test = inspect.getfile(Grader)
    path_for_test_folder = os.path.dirname(path_for_test)
    file_name = "standard.pkl"
    path_for_file = path_for_test_folder + "\\" + file_name
    #normalize file path
    path_for_file = os.path.normpath(path_for_file)
   
    #3. open file, get standard object 
    standard_file = open(path_for_file, "rb")
    standard_obj = pickle.load(standard_file)
    standard_file.close()
    #
    print("\tstandard object loaded")
    #
    if not standard_obj["testName"] == testName:
        c = "standard doesnt match test"
        raise Exception(c)
    #
    print("first, check whether build completed do() without generating an exception.")
    print("that is, check whether output is a descendant of the Exception class.")
    if result["errors"] != []: 
        print(".do() generated an exception. conclude verification.")
        result["completed"] = False
    else:
        result["completed"] = True
        print("build completed test.do() successfully.")
        print("compare output to standard.")
        passed, rubric = Grader.check(result, standard_obj)
        result["passed"] = passed
        result["rubric"] = rubric
        print("%s result: %s \n" % (testName,result["passed"]))
    return passed


    
